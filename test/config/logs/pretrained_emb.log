initialized exp_global.param_init: GlorotInitializer@140630520907704({})
initialized exp_global.bias_init: ZeroInitializer@140630520908768({})
initialized exp_global: ExpGlobal@140630520906696({'model_file': 'test/config/models/pretrained_emb.mod', 'log_file': 'test/config/logs/pretrained_emb.log', 'dropout': 0.3, 'weight_noise': 0.0, 'default_layer_dim': 512, 'param_init': GlorotInitializer@140630539313392, 'bias_init': ZeroInitializer@140630521547016, 'commandline_args': Namespace(dynet_autobatch=None, dynet_devices=None, dynet_gpu=False, dynet_gpu_ids=None, dynet_gpus=None, dynet_mem=None, dynet_profiling=None, dynet_seed=None, dynet_viz=False, dynet_weight_decay=None, experiment_name=[], experiments_file='test/config/pretrained_embeddings.yaml', generate_doc=False, settings='standard')})
initialized model.src_reader.vocab: Vocab@140630520909048({'vocab_file': 'examples/data/head.ja.vocab'})
initialized model.src_reader: PlainTextReader@140630539388856({'vocab': Vocab@140630540301368})
initialized model.trg_reader.vocab: Vocab@140630520907256({'vocab_file': 'examples/data/head.en.vocab'})
initialized model.trg_reader: PlainTextReader@140630520908824({'vocab': Vocab@140630540535008})
for model.src_embedder.vocab: reusing previously initialized Vocab@140630540301368
for model.src_embedder.src_reader: reusing previously initialized PlainTextReader@140630521546120
for model.src_embedder.trg_reader: reusing previously initialized PlainTextReader@140630539174744
65 vocabulary matches out of 66 total embeddings; 8 vocabulary words without a pretrained embedding out of 73
initialized model.src_embedder: PretrainedSimpleWordEmbedder@140630520907480({'filename': 'examples/data/wiki.ja.vec.small', 'emb_dim': 300, 'weight_noise': 0.0, 'vocab': Vocab@140630540301368, 'src_reader': PlainTextReader@140630521546120, 'trg_reader': PlainTextReader@140630539174744, 'yaml_path': model.src_embedder})
for model.encoder.weightnoise_std: reusing previously initialized 0.0
for model.encoder.param_init: reusing previously initialized GlorotInitializer@140630539313392
for model.encoder.bias_init: reusing previously initialized ZeroInitializer@140630521547016
initialized model.encoder: BiLSTMSeqTransducer@140630520905912({'layers': 1, 'input_dim': 300, 'hidden_dim': 512, 'dropout': 0.3, 'weightnoise_std': 0.0, 'param_init': GlorotInitializer@140630539313392, 'bias_init': ZeroInitializer@140630521547016})
for model.attender.param_init: reusing previously initialized GlorotInitializer@140630539313392
for model.attender.bias_init: reusing previously initialized ZeroInitializer@140630521547016
initialized model.attender: MlpAttender@140630520907088({'input_dim': 512, 'state_dim': 512, 'hidden_dim': 512, 'param_init': GlorotInitializer@140630539313392, 'bias_init': ZeroInitializer@140630521547016})
for model.trg_embedder.weight_noise: reusing previously initialized 0.0
for model.trg_embedder.param_init: reusing previously initialized GlorotInitializer@140630539313392
for model.trg_embedder.src_reader: reusing previously initialized PlainTextReader@140630521546120
for model.trg_embedder.trg_reader: reusing previously initialized PlainTextReader@140630539174744
initialized model.trg_embedder: SimpleWordEmbedder@140630520906472({'emb_dim': 512, 'weight_noise': 0.0, 'param_init': GlorotInitializer@140630539313392, 'src_reader': PlainTextReader@140630521546120, 'trg_reader': PlainTextReader@140630539174744, 'yaml_path': model.trg_embedder})
for model.decoder.rnn_layer.dropout: reusing previously initialized 0.3
for model.decoder.rnn_layer.weightnoise_std: reusing previously initialized 0.0
for model.decoder.rnn_layer.param_init: reusing previously initialized GlorotInitializer@140630539313392
for model.decoder.rnn_layer.bias_init: reusing previously initialized ZeroInitializer@140630521547016
initialized model.decoder.rnn_layer: UniLSTMSeqTransducer@140630520908208({'input_dim': 512, 'hidden_dim': 512, 'dropout': 0.3, 'weightnoise_std': 0.0, 'param_init': GlorotInitializer@140630539313392, 'bias_init': ZeroInitializer@140630521547016, 'decoder_input_dim': 512, 'yaml_path': model.decoder.rnn_layer})
for model.decoder.mlp_layer.output_dim: reusing previously initialized 512
for model.decoder.mlp_layer.param_init_hidden: reusing previously initialized GlorotInitializer@140630539313392
for model.decoder.mlp_layer.bias_init_hidden: reusing previously initialized ZeroInitializer@140630521547016
for model.decoder.mlp_layer.param_init_output: reusing previously initialized GlorotInitializer@140630539313392
for model.decoder.mlp_layer.bias_init_output: reusing previously initialized ZeroInitializer@140630521547016
for model.decoder.mlp_layer.trg_reader: reusing previously initialized PlainTextReader@140630539174744
initialized model.decoder.mlp_layer: MLP@140630520906192({'input_dim': 512, 'hidden_dim': 512, 'output_dim': 512, 'param_init_hidden': GlorotInitializer@140630539313392, 'bias_init_hidden': ZeroInitializer@140630521547016, 'param_init_output': GlorotInitializer@140630539313392, 'bias_init_output': ZeroInitializer@140630521547016, 'trg_reader': PlainTextReader@140630539174744, 'decoder_rnn_dim': 512, 'yaml_path': model.decoder.mlp_layer})
initialized model.decoder.bridge: CopyBridge@140630520906248({'dec_dim': 512})
initialized model.decoder: MlpSoftmaxDecoder@140630520908992({'input_dim': 512, 'trg_embed_dim': 512, 'rnn_layer': UniLSTMSeqTransducer@140630539316248, 'mlp_layer': MLP@140630539316416, 'bridge': CopyBridge@140630521544832})
initialized train.batcher: SrcBatcher@140630520908096({'batch_size': 32})
for model.inference.batcher: reusing previously initialized SrcBatcher@140630521545280
initialized model.inference: SimpleInference@140630520907648({'batcher': SrcBatcher@140630521545280})
initialized model: DefaultTranslator@140630539387904({'src_reader': PlainTextReader@140630521546120, 'trg_reader': PlainTextReader@140630539174744, 'src_embedder': PretrainedSimpleWordEmbedder@140630520908544, 'encoder': BiLSTMSeqTransducer@140630520963024, 'attender': MlpAttender@140630520962184, 'trg_embedder': SimpleWordEmbedder@140630520960112, 'decoder': MlpSoftmaxDecoder@140630520908208, 'inference': SimpleInference@140630520908096})
for train.model: reusing previously initialized DefaultTranslator@140630520908992
initialized train.trainer: AdamTrainer@140630520906360({'alpha': 0.0002})
for train.dev_tasks.0.model: reusing previously initialized DefaultTranslator@140630520908992
for train.dev_tasks.0.batcher: reusing previously initialized SrcBatcher@140630521545280
initialized train.dev_tasks.0: LossEvalTask@140630520907928({'src_file': 'examples/data/head.ja', 'ref_file': 'examples/data/head.en', 'model': DefaultTranslator@140630520908992, 'batcher': SrcBatcher@140630521545280})
initialized train: SimpleTrainingRegimen@140630520907816({'model': DefaultTranslator@140630520908992, 'src_file': 'examples/data/head.ja', 'trg_file': 'examples/data/head.en', 'batcher': SrcBatcher@140630521545280, 'trainer': AdamTrainer@140630539388856, 'run_for_epochs': 20, 'lr_decay': 0.5, 'dev_tasks': [LossEvalTask@140630520907088], 'restart_trainer': True, 'name': 'pretrained_emb', 'commandline_args': Namespace(dynet_autobatch=None, dynet_devices=None, dynet_gpu=False, dynet_gpu_ids=None, dynet_gpus=None, dynet_mem=None, dynet_profiling=None, dynet_seed=None, dynet_viz=False, dynet_weight_decay=None, experiment_name=[], experiments_file='test/config/pretrained_embeddings.yaml', generate_doc=False, settings='standard')})
initialized : Experiment@140630539387456({'exp_global': ExpGlobal@140630520962800, 'model': DefaultTranslator@140630520908992, 'train': SimpleTrainingRegimen@140630520910624})
> use randomly initialized DyNet weights of all components
> Training
[pretrained_emb] Epoch 1.0000: train_loss/word=4.840897 (words=91, words/sec=780.09, time=0-00:00:00)
> Checkpoint
[pretrained_emb] Epoch 1.0000 dev Loss: 4.662 (words=91, words/sec=779.77, time=0-00:00:00)
Epoch 1.0000: best dev score, writing out model
=> Running prior-segmenting
running XNMT revision 48eb2a8 on ahctitan02 on 2018-04-10 18:30:53
